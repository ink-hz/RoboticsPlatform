#!/usr/bin/env python3
"""
MVPå¹³å°æ€§èƒ½æµ‹è¯•è„šæœ¬
"""

import asyncio
import aiohttp
import time
import json
import statistics
from concurrent.futures import ThreadPoolExecutor
import threading
from datetime import datetime
import random

class PerformanceTestSuite:
    """æ€§èƒ½æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, base_url="http://localhost:8080", max_concurrent=100):
        self.base_url = base_url
        self.max_concurrent = max_concurrent
        self.results = {
            'latencies': [],
            'throughput': 0,
            'error_rate': 0,
            'success_count': 0,
            'error_count': 0,
            'start_time': None,
            'end_time': None
        }
        self.lock = threading.Lock()
    
    async def send_request(self, session, method, endpoint, data=None):
        """å‘é€å¼‚æ­¥è¯·æ±‚"""
        url = f"{self.base_url}{endpoint}"
        start_time = time.time()
        
        try:
            if method.upper() == 'POST':
                async with session.post(url, json=data) as response:
                    await response.text()
                    status = response.status
            else:
                async with session.get(url) as response:
                    await response.text()
                    status = response.status
            
            latency = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            with self.lock:
                self.results['latencies'].append(latency)
                if status == 200:
                    self.results['success_count'] += 1
                else:
                    self.results['error_count'] += 1
            
            return status == 200, latency
            
        except Exception as e:
            latency = (time.time() - start_time) * 1000
            with self.lock:
                self.results['error_count'] += 1
                self.results['latencies'].append(latency)
            return False, latency
    
    async def data_ingestion_load_test(self, concurrent_users=50, duration_seconds=60):
        """æ•°æ®æ‘„å…¥è´Ÿè½½æµ‹è¯•"""
        print(f"ğŸš€ Starting data ingestion load test...")
        print(f"   Concurrent users: {concurrent_users}")
        print(f"   Duration: {duration_seconds}s")
        
        self.results['start_time'] = time.time()
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            end_time = time.time() + duration_seconds
            
            while time.time() < end_time:
                # æ§åˆ¶å¹¶å‘æ•°é‡
                if len(tasks) >= concurrent_users:
                    done, tasks = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
                
                # ç”Ÿæˆæµ‹è¯•æ•°æ®
                device_id = f"perf_test_{random.randint(1, 100)}"
                sensor_data = {
                    "device_id": device_id,
                    "sensor_type": random.choice(["temperature", "humidity", "pressure", "cpu"]),
                    "value": random.uniform(0, 100),
                    "metadata": {
                        "test_type": "performance",
                        "timestamp": datetime.now().isoformat()
                    }
                }
                
                # åˆ›å»ºä»»åŠ¡
                task = asyncio.create_task(
                    self.send_request(session, 'POST', '/api/data/ingest', sensor_data)
                )
                tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            if tasks:
                await asyncio.wait(tasks)
        
        self.results['end_time'] = time.time()
        self.calculate_metrics()
        
        return self.results
    
    async def mixed_workload_test(self, concurrent_users=30, duration_seconds=120):
        """æ··åˆå·¥ä½œè´Ÿè½½æµ‹è¯•"""
        print(f"ğŸ”„ Starting mixed workload test...")
        print(f"   Concurrent users: {concurrent_users}")
        print(f"   Duration: {duration_seconds}s")
        
        self.results['start_time'] = time.time()
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            end_time = time.time() + duration_seconds
            
            while time.time() < end_time:
                if len(tasks) >= concurrent_users:
                    done, tasks = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
                
                # éšæœºé€‰æ‹©æ“ä½œç±»å‹
                operation = random.choices(
                    ['ingest', 'query', 'analytics', 'metrics'],
                    weights=[60, 25, 10, 5]  # æ‘„å…¥60%, æŸ¥è¯¢25%, åˆ†æ10%, æŒ‡æ ‡5%
                )[0]
                
                if operation == 'ingest':
                    sensor_data = {
                        "device_id": f"mixed_test_{random.randint(1, 50)}",
                        "sensor_type": random.choice(["temp", "humid", "pressure"]),
                        "value": random.uniform(10, 90),
                        "metadata": {"test": "mixed_workload"}
                    }
                    task = asyncio.create_task(
                        self.send_request(session, 'POST', '/api/data/ingest', sensor_data)
                    )
                elif operation == 'query':
                    device_id = f"mixed_test_{random.randint(1, 50)}"
                    task = asyncio.create_task(
                        self.send_request(session, 'GET', f'/api/data/{device_id}?limit=10')
                    )
                elif operation == 'analytics':
                    task = asyncio.create_task(
                        self.send_request(session, 'GET', '/api/analytics/summary')
                    )
                else:  # metrics
                    task = asyncio.create_task(
                        self.send_request(session, 'GET', '/api/metrics')
                    )
                
                tasks.append(task)
            
            if tasks:
                await asyncio.wait(tasks)
        
        self.results['end_time'] = time.time()
        self.calculate_metrics()
        
        return self.results
    
    async def spike_test(self, spike_users=200, spike_duration=30):
        """å³°å€¼è´Ÿè½½æµ‹è¯•"""
        print(f"âš¡ Starting spike test...")
        print(f"   Spike users: {spike_users}")
        print(f"   Spike duration: {spike_duration}s")
        
        self.results['start_time'] = time.time()
        
        async with aiohttp.ClientSession() as session:
            # åˆ›å»ºå¤§é‡å¹¶å‘è¯·æ±‚
            tasks = []
            
            for i in range(spike_users):
                sensor_data = {
                    "device_id": f"spike_test_{i}",
                    "sensor_type": "load_test",
                    "value": random.uniform(0, 100),
                    "metadata": {"test": "spike", "user_id": i}
                }
                
                task = asyncio.create_task(
                    self.send_request(session, 'POST', '/api/data/ingest', sensor_data)
                )
                tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
            await asyncio.wait(tasks)
        
        self.results['end_time'] = time.time()
        self.calculate_metrics()
        
        return self.results
    
    def calculate_metrics(self):
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        if not self.results['latencies']:
            return
        
        duration = self.results['end_time'] - self.results['start_time']
        total_requests = self.results['success_count'] + self.results['error_count']
        
        self.results['throughput'] = total_requests / duration if duration > 0 else 0
        self.results['error_rate'] = (self.results['error_count'] / total_requests * 100) if total_requests > 0 else 0
        self.results['avg_latency'] = statistics.mean(self.results['latencies'])
        self.results['p50_latency'] = statistics.median(self.results['latencies'])
        self.results['p95_latency'] = statistics.quantiles(self.results['latencies'], n=20)[18] if len(self.results['latencies']) > 20 else max(self.results['latencies'])
        self.results['p99_latency'] = statistics.quantiles(self.results['latencies'], n=100)[98] if len(self.results['latencies']) > 100 else max(self.results['latencies'])
        self.results['max_latency'] = max(self.results['latencies'])
        self.results['min_latency'] = min(self.results['latencies'])
    
    def print_results(self, test_name="Performance Test"):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š {test_name} Results")
        print(f"{'='*60}")
        print(f"ğŸ• Duration: {self.results['end_time'] - self.results['start_time']:.2f}s")
        print(f"ğŸ“ˆ Throughput: {self.results['throughput']:.2f} req/sec")
        print(f"âœ… Successful requests: {self.results['success_count']}")
        print(f"âŒ Failed requests: {self.results['error_count']}")
        print(f"ğŸ“‰ Error rate: {self.results['error_rate']:.2f}%")
        print(f"\nâ±ï¸  Latency Statistics (ms):")
        print(f"   Average: {self.results.get('avg_latency', 0):.2f}")
        print(f"   P50 (Median): {self.results.get('p50_latency', 0):.2f}")
        print(f"   P95: {self.results.get('p95_latency', 0):.2f}")
        print(f"   P99: {self.results.get('p99_latency', 0):.2f}")
        print(f"   Min: {self.results.get('min_latency', 0):.2f}")
        print(f"   Max: {self.results.get('max_latency', 0):.2f}")
        print(f"{'='*60}")
    
    def reset_results(self):
        """é‡ç½®æµ‹è¯•ç»“æœ"""
        self.results = {
            'latencies': [],
            'throughput': 0,
            'error_rate': 0,
            'success_count': 0,
            'error_count': 0,
            'start_time': None,
            'end_time': None
        }

async def run_comprehensive_performance_test():
    """è¿è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•"""
    print("ğŸ¯ MVP Platform Comprehensive Performance Testing")
    print("="*60)
    
    # å¥åº·æ£€æŸ¥
    tester = PerformanceTestSuite()
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(f"{tester.base_url}/health") as response:
                if response.status != 200:
                    print("âŒ Platform health check failed")
                    return
        except Exception as e:
            print(f"âŒ Cannot connect to platform: {e}")
            return
    
    print("âœ… Platform health check passed")
    
    # æµ‹è¯•åºåˆ—
    tests = [
        ("Data Ingestion Load Test", tester.data_ingestion_load_test, {"concurrent_users": 20, "duration_seconds": 30}),
        ("Mixed Workload Test", tester.mixed_workload_test, {"concurrent_users": 15, "duration_seconds": 60}),
        ("Spike Test", tester.spike_test, {"spike_users": 100, "spike_duration": 20}),
    ]
    
    all_results = {}
    
    for test_name, test_func, test_params in tests:
        print(f"\nğŸš¦ Starting {test_name}...")
        tester.reset_results()
        
        try:
            results = await test_func(**test_params)
            tester.print_results(test_name)
            all_results[test_name] = results.copy()
        except Exception as e:
            print(f"âŒ {test_name} failed: {e}")
        
        # æµ‹è¯•é—´éš”
        print("\nâ³ Cooling down for 10 seconds...")
        await asyncio.sleep(10)
    
    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    generate_performance_report(all_results)

def generate_performance_report(results):
    """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"performance_report_{timestamp}.json"
    
    report_data = {
        "timestamp": datetime.now().isoformat(),
        "test_results": results,
        "summary": {
            "total_tests": len(results),
            "avg_throughput": sum(r.get('throughput', 0) for r in results.values()) / len(results) if results else 0,
            "avg_error_rate": sum(r.get('error_rate', 0) for r in results.values()) / len(results) if results else 0,
        }
    }
    
    with open(report_file, 'w') as f:
        json.dump(report_data, f, indent=2)
    
    print(f"\nğŸ“„ Performance report saved to: {report_file}")
    
    # æ‰“å°æ±‡æ€»
    print(f"\nğŸ¯ Overall Performance Summary")
    print(f"{'='*40}")
    print(f"Average Throughput: {report_data['summary']['avg_throughput']:.2f} req/sec")
    print(f"Average Error Rate: {report_data['summary']['avg_error_rate']:.2f}%")

if __name__ == "__main__":
    try:
        asyncio.run(run_comprehensive_performance_test())
    except KeyboardInterrupt:
        print("\nğŸ›‘ Performance test interrupted by user")
    except Exception as e:
        print(f"\nâŒ Performance test failed: {e}")